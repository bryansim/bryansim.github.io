{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Shapley Value Regression\n",
    "\n",
    "When we have many variables predicting an outcome, how do we know which is the most \"important\"? In a multiple regression context, one might think, intuitively, that if you threw all the predictors into the same model, the one with the largest coefficient woud be the most important predictor.\n",
    "\n",
    "If all the predictors were perfectly orthogonal (i.e., completely uncorrelated), this would be true, as the coefficients would not change as a function of which predictors were included/excluded in the model.\n",
    "\n",
    "However, in most applications (perhaps even the vast majority of real-world cases), predictors are correlated with one another. This results in overlap in the variance that each predictor explains, and confounds simple interpretation of coefficients and R<sup>2</sup> statistics *with regard to* judging how important one predictor is over another. One consequence of predictors being correlated, for example, is that the order in which predictors are entered into the model affects the degree to which they increase R<sup>2</sup>.\n",
    "\n",
    "One solution to this problem is, when dealing with multiple predictors, to iterate over all possible orderings of the predictors, and obtain the *average* importance of each predictor. This procedure is known as Shapley value regression, and is detailed [here](http://www.r-bloggers.com/the-relative-importance-of-predictors-let-the-games-begin/).\n",
    "\n",
    "The R package relaimpo does this, but I wanted to work out an analog in Python."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import itertools\n",
    "\n",
    "class fit:\n",
    "    '''\n",
    "    This creates an instance of the class OLS with attributes coefficients, fitted_values, residuals, and rsquared \n",
    "    '''\n",
    "    def __init__(self, data, outcome, predictors):\n",
    "        #We do the following so we can call them as attributes\n",
    "        self.outcome = outcome\n",
    "        self.predictors = {key: value for (key,value) in [(x, 0) for x in predictors]}\n",
    "        #Create some dictionaries with all the predictors as keys, and empty lists as values\n",
    "        self.resultsrsquared = {k:[] for k in predictors} \n",
    "        self.resultscoefficients = {k:[] for k in predictors}\n",
    "        self.results = {k:[] for k in predictors}\n",
    "        #The above is preferred over dict.fromkeys(list,default) as the default value is shared among all keys, so appending an object\n",
    "        #will append it to all keys.\n",
    "        #See: http://stackoverflow.com/questions/23397153/append-value-to-one-list-in-dictionary-appends-value-to-all-lists-in-dictionary\n",
    "        self.iterate(data, outcome, predictors)\n",
    "    \n",
    "    '''\n",
    "    The method below returns the coefficients from a least-squares regression.\n",
    "    '''    \n",
    "    def ols(self, Y, X): #Y is an nx1 vector, X is an nxp matrix\n",
    "        X_transpose = np.transpose(X)\n",
    "        X_transpose_X = np.dot(X_transpose, X)\n",
    "        X_inverse = np.linalg.inv(X_transpose_X)\n",
    "        X_transpose_Y = np.dot(X_transpose, Y)\n",
    "               \n",
    "        self.coefficients = np.dot(X_inverse, X_transpose_Y)\n",
    "        self.intercept = float(self.coefficients[0])\n",
    "        self.coefficientslist = list(self.coefficients[1:]) #Change type to list\n",
    "        \n",
    "        '''\n",
    "        The lines below are for calculating R^2.\n",
    "        '''\n",
    "        self.fitted_values = np.dot(X, self.coefficients)\n",
    "        self.residuals = Y - self.fitted_values        \n",
    "        \n",
    "        #SStotal\n",
    "        yty = np.dot(np.transpose(Y),Y)\n",
    "        j = np.ones((len(Y),len(Y)))\n",
    "        ytj = np.dot(np.transpose(Y), j)\n",
    "        ytj = np.dot(ytj,Y)/len(Y)\n",
    "        sst = yty - ytj\n",
    "        #SSerror\n",
    "        sse = np.dot(np.transpose(self.residuals), self.residuals)\n",
    "        #SSregression\n",
    "        ssr = sst - sse\n",
    "        #rsquared\n",
    "        self.rsquared = ssr/sst\n",
    "        return self.coefficientslist\n",
    "        \n",
    "    def iterate(self,data, outcome, predictors):\n",
    "        #Create dataset where we remove missing values listwise\n",
    "        #SOMETHING IS WRONG WITH HOW YOU ARE SELECTING COLUMNS\n",
    "        Y = np.array(data['{0}'.format(outcome)])\n",
    "        \n",
    "        print \"Total cases: {0}\".format(len(Y))\n",
    "        \n",
    "        all_data = np.column_stack((Y, np.ones((len(Y), 1))))\n",
    "        for bs in predictors:\n",
    "            all_data =  np.column_stack((all_data, np.array(data[bs])))\n",
    "        all_data = all_data[~np.isnan(all_data).any(axis=1)]\n",
    "        Y = all_data[:,0]\n",
    "        C = all_data[:,1]\n",
    "        X = all_data[:,2:]\n",
    "        \n",
    "        print \"Cases after listwise deletion: {0}\".format(len(Y))   \n",
    "        \n",
    "        '''\n",
    "        Iterate through all predictors and return the average coefficient for all combinations of predictors\n",
    "        '''\n",
    "        for predictor in range(0, len(predictors)): #for each predictor, let's find the average for 1 to n predictors\n",
    "        \n",
    "            otherpredictors = [oth for oth in range(0,len(predictors)) if oth != predictor] #list of other predictors\n",
    "            dictionaryofcoefficients = {n:[] for n in range(1,len(predictors)+1)} # +1 to reflect 1 to n predictors rather than 0 to n-1\n",
    "            dictionaryofrsquared = {n:[] for n in range(1,len(predictors)+1)}\n",
    "            \n",
    "            Xpredictor = np.column_stack((C,X[:,predictor]))\n",
    "\n",
    "            for npred in range(0,len(predictors)): #from 1 to n-1 other predictors\n",
    "                ordering = itertools.combinations(otherpredictors, npred)\n",
    "                for comb in ordering:\n",
    "                    #do ols for y and our subset of X\n",
    "                    if len(comb) == 0:\n",
    "                        dictionaryofcoefficients[npred+1].append(self.ols(Y, Xpredictor)[0]) \n",
    "                        dictionaryofrsquared[npred+1].append(self.rsquared)\n",
    "                    else:\n",
    "                        Xsub = X[:,list(comb)] #the other predictors are in this matrix\n",
    "                        Xsub = np.column_stack((Xpredictor,Xsub)) #combine them\n",
    "                        dictionaryofcoefficients[npred+1].append(self.ols(Y, Xsub)[0]) \n",
    "                        dictionaryofrsquared[npred+1].append(self.rsquared)\n",
    "            \n",
    "            dictionaryofrsquared.pop(len(predictors)) #because the model with all predictors always has the same R squared\n",
    "            \n",
    "            for k,v in dictionaryofcoefficients.iteritems():\n",
    "                dictionaryofcoefficients[k] = sum(dictionaryofcoefficients[k])/len(dictionaryofcoefficients[k])\n",
    "                self.resultscoefficients[predictors[predictor]] = dictionaryofcoefficients\n",
    "            \n",
    "            for k,v in sorted(dictionaryofrsquared.iteritems(), reverse = True):    \n",
    "                if k == 1:\n",
    "                    dictionaryofrsquared[k] = v[0]\n",
    "                else:\n",
    "                    dictionaryofrsquared[k] = (sum(dictionaryofrsquared[k])/len(dictionaryofrsquared[k])) \n",
    "                    #- (sum(dictionaryofrsquared[k-1])/len(dictionaryofrsquared[k-1]))\n",
    "                self.resultsrsquared[predictors[predictor]] = dictionaryofrsquared\n",
    "            \n",
    "        for k, v in self.resultsrsquared.iteritems():\n",
    "            self.results[k] = sum(v.values())/len(v.values())\n",
    "             "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As far as I can tell, this code does about 80% of what I want it to. It returns a list of what the average coefficient for each predictor is as a function of how many other predictors are in the model, and also returns the average R<sup>2</sup> of the model with N predictors in it. There's still more to do though:\n",
    "\n",
    "1. The code runs extremely slow, probably because of the weird nested for-loops I have in there.\n",
    "2. The method doesn't actually return the relative importance of each predictor. This has to do with the average change in R<sup>2</sup> that each predictor contributes when added to the model. Right now, it just returns the R<sup>2</sup> of the whole model.\n",
    "\n",
    "I'll be working on this in the coming days, but for now, as an example for what does (and does not) work, let's run this on the Skytrax data, and see what happens:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import psycopg2\n",
    "import pandas as pd\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#get data from postgres database\n",
    "connection = psycopg2.connect(\"dbname = skytrax user=skytraxadmin password=skytraxadmin\")\n",
    "cursor = connection.cursor()\n",
    "cursor.execute(\"SELECT * from airline\")\n",
    "testdata = cursor.fetchall()\n",
    "column_names = [desc[0] for desc in cursor.description]\n",
    "testdata = pd.DataFrame(testdata)\n",
    "testdata.columns = column_names"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total cases: 41396\n",
      "Cases after listwise deletion: 28341\n",
      "\n",
      "Average coefficients with N other predictors in the model:\n",
      "\n",
      "{'value_money_rating': {1: 1.8649063450094783, 2: 1.5144640753771565, 3: 1.2811039278852754, 4: 1.1094616213211062, 5: 0.97588253839017369}, 'cabin_staff_rating': {1: 1.7220375706163686, 2: 1.2423932090274965, 3: 0.96482096920203941, 4: 0.78336800727344302, 5: 0.66212106011664673}, 'food_beverages_rating': {1: 1.3824000680960342, 2: 0.74574656752576396, 3: 0.41993379259250335, 4: 0.25813695445838092, 5: 0.16598034625130431}, 'seat_comfort_rating': {1: 1.7279134729889485, 2: 1.103370681632768, 3: 0.76087228190797218, 4: 0.55735092810755438, 5: 0.42553696755928527}, 'inflight_entertainment_rating': {1: 0.81813312432369223, 2: 0.25077293584225169, 3: 0.10988060366414072, 4: 0.055685681377817153, 5: 0.031685430819348603}}\n",
      "\n",
      "Average Rsquared with N other predictors in the model:\n",
      "\n",
      "{'value_money_rating': {1: 0.70000482291020016, 2: 0.73437848675413542, 3: 0.75769834885876719, 4: 0.77509515934098283}, 'cabin_staff_rating': {1: 0.61108838008392297, 2: 0.68880734158425838, 3: 0.73593199899832795, 4: 0.76760232624119729}, 'food_beverages_rating': {1: 0.41976593597077677, 2: 0.60519081823348375, 3: 0.70695963415469354, 4: 0.7590299721429592}, 'seat_comfort_rating': {1: 0.52806397291997464, 2: 0.64985921373852684, 3: 0.71984029509356728, 4: 0.76190216873812322}, 'inflight_entertainment_rating': {1: 0.1814590593325032, 2: 0.5789830526105908, 3: 0.70255932748876526, 4: 0.75836033918843182}}\n",
      "\n",
      "This crap took 255.414000034s to run\n"
     ]
    }
   ],
   "source": [
    "import time\n",
    "import Shapley\n",
    "reload(Shapley)\n",
    "\n",
    "start = time.time()\n",
    "\n",
    "fit = Shapley.fit(testdata, \"overall_rating\", ['cabin_staff_rating', 'seat_comfort_rating', 'food_beverages_rating', \\\n",
    "                                                    'inflight_entertainment_rating', 'value_money_rating'])\n",
    "\n",
    "print \"\\nAverage coefficients with N other predictors in the model:\\n\"\n",
    "print fit.resultscoefficients\n",
    "print \"\\nAverage Rsquared with N other predictors in the model:\\n\"\n",
    "print fit.resultsrsquared\n",
    "print \"\\nThis crap took {0}s to run\".format(time.time() - start)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.11"
  },
  "nikola": {
   "category": "",
   "date": "2016-05-05 19:51:29 UTC-04:00",
   "description": "",
   "link": "",
   "slug": "skytrax-data-2-shapley-value-regression",
   "tags": "skytrax",
   "title": "Skytrax Data #2: Shapley Value Regression",
   "type": "text"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
